{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento de texto NLP SIN USAR LIBRERIAS\n",
        "**By Jean Carlo Alvarez Ramirez**\n",
        "\n",
        "En la demo vamos a ver:\n",
        "\n",
        "Limpieza, la remoción del contenido no deseado\n",
        "\n",
        "Normalización, la conversión diferentes formas a una sola\n",
        "\n",
        "Tokenización, la separación del texto en tókenes (unidades mínimas, por ejemplo palabras)\n",
        "\n",
        "Separación en conjuntos de datos: entrenamiento, validación, prueba\n",
        "\n",
        "Generación del vocabulario, la lista de tókenes conocidos\n",
        "\n",
        "Creacion de embeddings"
      ],
      "metadata": {
        "id": "h6p6m4qUHj0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerias"
      ],
      "metadata": {
        "id": "2lb_ZIylHnMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhJVL2YFHe6R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import re**\n",
        "Importa el módulo re que proporciona funciones para trabajar con expresiones regulares en Python\n",
        "\n",
        "**¿Para qué sirve?**\n",
        "Permite realizar tareas como búsqueda, coincidencia y manipulación de cadenas de texto utilizando patrones\n",
        "\n",
        "**import string**\n",
        "Importa el módulo string que contiene constantes y funciones relacionadas con la manipulación de cadenas de texto\n",
        "\n",
        "**¿Para qué sirve?**\n",
        "Ofrece listas predefinidas de caracteres comunes, como letras, dígitos y símbolos\n",
        "\n",
        "**import random**\n",
        "Importa el módulo random que permite generar números aleatorios y realizar selecciones al azar\n",
        "\n",
        "**¿Para qué sirve?**\n",
        "Es útil para tareas como crear datos aleatorios, barajar listas o realizar simulaciones\n",
        "\n",
        "**import numpy as np**\n",
        "Importa la librería numpy y la renombra como np por convención\n",
        "\n",
        "**¿Para qué sirve?**\n",
        "Es una librería clave para cálculos científicos y operaciones con matrices y arreglos multidimensionales\n",
        "\n",
        "**from sklearn.model_selection import train_test_split**\n",
        "\n",
        "Importa la función train_test_split desde el módulo sklearn.model_selection\n",
        "\n",
        "**¿Para qué sirve?**\n",
        "\n",
        "Divide un conjunto de datos en subconjuntos de entrenamiento y prueba para entrenar modelos de Machine Learning\n",
        "\n",
        "**Parámetros clave:**\n",
        "- X: Características (features) de entrada\n",
        "- y: Etiquetas (targets)\n",
        "- test_size: Proporción de datos para prueba (ej: 0.2 usa el 20% para testing)\n",
        "- random_state: Fija la semilla para resultados reproducibles\n",
        "- shuffle: Si es True (por defecto), mezcla los datos antes de dividir\n",
        "\n",
        "**¿Qué pasa si cambia algo?**\n",
        "- test_size afecta el balance entre datos para entrenar y probar\n",
        "- random_state garantiza resultados iguales en ejecuciones repetidas\n",
        "- shuffle=False puede ser útil si los datos tienen un orden temporal"
      ],
      "metadata": {
        "id": "RATmrj-1HmuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpiar y Normalizar texto\n",
        "\n",
        "La función limpiar_texto recibe un string y realiza varias operaciones:\n",
        "Convierte el texto a minúsculas para unificar la representación.\n",
        "Elimina dígitos y signos de puntuación utilizando expresiones regulares.\n",
        "Quita espacios innecesarios al inicio y final del texto.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E4pCriSsHsGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "      - Convierte a minúsculas\n",
        "      - Elimina dígitos\n",
        "      - Elimina signos de puntuación\n",
        "      - Elimina espacios extra al inicio y al final\n",
        "    \"\"\"\n",
        "    texto = texto.lower()  # Normalizamos a minúsculas\n",
        "    texto = re.sub(r'\\d+', '', texto)  # Eliminamos dígitos\n",
        "    # Eliminamos signos de puntuación utilizando la librería string\n",
        "    texto = re.sub(r'[' + re.escape(string.punctuation) + ']', '', texto)\n",
        "    texto = texto.strip()  # Quitamos espacios al inicio y al final\n",
        "    return texto"
      ],
      "metadata": {
        "id": "4bZ8LdjZHxjW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def limpiar_texto(texto):**\n",
        "\n",
        "Define una función llamada limpiar_texto que recibe un parámetro texto, que debe ser una cadena de caracteres (str) esta se encarga de limpiar y normalizar el texto y devolverlo una vez limpio y normalizado.\n",
        "\n",
        "**texto.lower()**\n",
        "Convierte todos los caracteres de la cadena a minúsculas\n",
        "\n",
        "**¿Por qué es útil?**\n",
        "Evita problemas de sensibilidad entre mayúsculas y minúsculas al comparar textos\n",
        "\n",
        "**re.sub(r'\\d+', '', texto)** - **re.sub(pattern, repl, string)**\n",
        "\n",
        "Reemplaza todas las coincidencias del patrón por la cadena repl\n",
        "\n",
        "- r'\\d+' Es una expresión regular que:\n",
        "\n",
        "    - \\d coincide con cualquier dígito (0-9)\n",
        "    - \"+\" significa uno o más dígitos consecutivos.\n",
        "    - '' Indica que se reemplazan los dígitos encontrados por una cadena vacía, es decir, se eliminan\n",
        "\n",
        "**re.sub(r'[' + re.escape(string.punctuation) + ']', '', texto)**\n",
        "\n",
        "Crea un patrón que coincide con cualquiera de los signos de puntuación.\n",
        "\n",
        "**¿Qué hace la línea?**\n",
        "Elimina todos los signos de puntuación del texto\n",
        "\n",
        "- string.punctuation : Contiene todos los signos de puntuación estándar ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "- re.escape(string.punctuation) : Añade barras invertidas a los caracteres especiales para que sean interpretados literalmente por la expresión regular\n",
        "\n",
        "**texto.strip()**\n",
        "\n",
        "Elimina espacios en blanco al inicio y al final de la cadena.\n",
        "También elimina caracteres como \\n (saltos de línea) o \\t (tabulaciones).\n"
      ],
      "metadata": {
        "id": "jOrW8MJHH5Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizar\n",
        "\n",
        "Separa el texto limpio en tokens (palabras) utilizando la función split(), que separa por espacios.\n",
        "En aplicaciones reales se pueden usar tokenizadores más robustos (por ejemplo, NLTK o spaCy)."
      ],
      "metadata": {
        "id": "bWZ1u8x9H5bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar_texto(texto):\n",
        "    \"\"\"\n",
        "    Separa el texto en tokens utilizando la separación por espacios.\n",
        "    En casos reales se podría usar una librería especializada (NLTK, spaCy, etc.)\n",
        "    \"\"\"\n",
        "    tokens = texto.split()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "TrtHwttKH85S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def tokenizar_texto(texto):**\n",
        "\n",
        "Define una función llamada tokenizar_texto que recibe un parámetro texto, que debe ser una cadena de caracteres (str) el mismo se encarga de realizar la tokenizacion y devolver el texto en tokens.\n",
        "\n",
        "**texto.split()**\n",
        "El método split() divide la cadena en una lista de palabras, usando los espacios en blanco como separador por defecto\n",
        "\n",
        "**¿Qué hace exactamente?**\n",
        "\n",
        "- Cada vez que encuentra un espacio, crea un nuevo elemento en la lista\n",
        "- También considera tabulaciones (\\t) y saltos de línea (\\n) como separadores\n",
        "- Elimina automáticamente múltiples espacios consecutivos y los trata como uno solo\n",
        "\n",
        "**¿Qué debes tener en cuenta?**\n",
        "\n",
        "La función básica no elimina signos de puntuación. Por ejemplo:\n",
        "\n",
        "El token \"mundo!\" mantiene el signo de exclamación\n",
        "Si necesitas eliminar la puntuación antes de tokenizar, puedes usar la función limpiar_texto que revisamos antes\n",
        "\n",
        "**¿Qué pasa si cambiamos algo?**\n",
        "\n",
        "- Si usas split(',') en lugar de split(): Separará por comas en vez de espacios\n",
        "\n",
        "- Si el texto tiene saltos de línea o tabulaciones: También serán tratados como separadores\n",
        "."
      ],
      "metadata": {
        "id": "f9UdzZr0Ryx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creamos vocabulario\n",
        "\n",
        "Se recorre el conjunto de entrenamiento para contar la frecuencia de cada token.\n",
        "Se crea un diccionario que asigna un identificador numérico a cada token.\n",
        "Se incluyen tokens especiales:\n",
        "\n",
        "- PAD con valor 0 para representar padding\n",
        "\n",
        "- UNK para tokens desconocidos que no aparecen en el vocabulario\n",
        "\n",
        "Se puede establecer un parámetro min_freq para filtrar tokens poco frecuentes"
      ],
      "metadata": {
        "id": "Jz80Eh0MIYWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_vocabulario(tokenized_texts, min_freq=1):\n",
        "    \"\"\"\n",
        "    Generamos un diccionario que mapea cada token a un número entero\n",
        "    Se pueden aplicar filtros de frecuencia mínima para eliminar tokens raros\n",
        "    Se agregan tokens especiales: <PAD> para padding y <UNK> para tokens desconocidos.\n",
        "    \"\"\"\n",
        "    token_freq = {}\n",
        "    for tokens in tokenized_texts:\n",
        "        for token in tokens:\n",
        "            token_freq[token] = token_freq.get(token, 0) + 1\n",
        "\n",
        "    # Inicializamos el vocabulario con tokens especiales\n",
        "    vocab = {\"<PAD>\": 0}  # 0 se reserva para el padding\n",
        "    next_id = 1\n",
        "    vocab[\"<UNK>\"] = next_id  # Token para palabras desconocidas\n",
        "    next_id += 1\n",
        "\n",
        "    # Agregamos tokens que cumplen con la frecuencia mínima\n",
        "    for token, freq in token_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[token] = next_id\n",
        "            next_id += 1\n",
        "\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "BN-qAMNDIgBV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def generar_vocabulario(tokenized_texts, min_freq=1):**\n",
        "\n",
        "Define una función llamada generar_vocabulario que:\n",
        "\n",
        "\n",
        "- Recibe tokenized_texts (una lista de listas de tokens)\n",
        "- Usa min_freq como frecuencia mínima para incluir un token en el vocabulario (por defecto es 1)\n",
        "\n",
        "**token_freq = {}**\n",
        "\n",
        "Inicializa un diccionario vacío para almacenar la frecuencia de cada token\n",
        "\n",
        "**for tokens in tokenized_texts:**\n",
        "Itera sobre cada lista de tokens (cada \"oración\" o \"texto\" ya tokenizado)\n",
        "\n",
        "**for token in tokens:**\n",
        "Itera sobre cada token individual dentro de la lista actual\n",
        "\n",
        "**token_freq.get(token, 0) + 1**\n",
        "\n",
        "- token_freq.get(token, 0) obtiene la frecuencia actual del token o usa 0 si el token no existe aún en el diccionario\n",
        "\n",
        "- Suma 1 para contar el token actual\n",
        "\n",
        "Al final de este bucle, token_freq contendrá la cantidad de veces que aparece cada token en el conjunto de textos\n",
        "\n",
        "**vocab = {\"<PAD>\": 0}**\n",
        "Crea un diccionario vocab donde:\n",
        "\n",
        "- \"PAD\" está mapeado al índice 0 (por convención en muchas redes neuronales para hacer padding)\n",
        "\n",
        "- next_id = 1 : Establece el siguiente ID disponible para otros tokens\n",
        "\n",
        "**vocab[\"UNK\"] = next_id**\n",
        "- Asigna el índice 1 al token especial \"UNK\", que se usará para palabras desconocidas\n",
        "\n",
        "- next_id += 1:\n",
        "Incrementa el contador para asignar IDs únicos a los siguientes tokens\n",
        "\n",
        "**for token, freq in token_freq.items():**\n",
        "\n",
        "Itera sobre cada token y su frecuencia en token_freq.\n",
        "\n",
        "**if freq >= min_freq:**\n",
        "\n",
        "Verifica si la frecuencia del token es igual o superior a min_freq.\n",
        "\n",
        "**vocab[token] = next_id**\n",
        "\n",
        "Si el token pasa el filtro de frecuencia, se le asigna un ID único.\n",
        "\n",
        "**next_id += 1**\n",
        "Se incrementa el contador para el siguiente token\n",
        "\n",
        "**¿Qué pasa si cambias algo?**\n",
        "\n",
        "- min_freq=1 → Incluye todos los tokens (excepto los que nunca aparecen).\n",
        "- min_freq=3 → Excluye tokens con baja frecuencia.\n",
        "- Si cambias el índice de PAD o UNK:\n",
        "Podría afectar modelos que dependen de esos valores específicos"
      ],
      "metadata": {
        "id": "iMjTbtM4Ioqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear Embeddings\n",
        "\n",
        "Se genera un vector aleatorio (de dimensión 50 en este ejemplo) para cada token del vocabulario.\n",
        "Estos vectores son útiles como representación de palabras en modelos de NLP. En la práctica se pueden usar embeddings preentrenados como Word2Vec o GloVe"
      ],
      "metadata": {
        "id": "ZyfGQYynIo_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_embeddings(vocab, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Creamos embedding para cada token del vocabulario\n",
        "    Se generan vectores aleatorios de dimensión embedding_dim para ilustrar el concepto\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    np.random.seed(42)  # Para reproducibilidad\n",
        "    for token in vocab:\n",
        "        embeddings[token] = np.random.rand(embedding_dim)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "7x0v3Z6bIxcM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def crear_embeddings(vocab, embedding_dim=50):**\n",
        "\n",
        "Define una función llamada crear_embeddings que:\n",
        "\n",
        "- vocab → es un diccionario que mapea tokens a números enteros (creado por la función generar_vocabulario)\n",
        "- embedding_dim=50 → define la dimensión del vector de embedding (por defecto es 50)\n",
        "\n",
        "**embeddings = {}**\n",
        "Inicializa un diccionario vacío llamado embeddings que almacenará el token como clave y su vector como valor\n",
        "\n",
        "**np.random.seed(42)**\n",
        "\n",
        "Fija la semilla aleatoria de NumPy para que los resultados sean reproducibles\n",
        "\n",
        "- Cada vez que ejecutes la función, obtendrás los mismos números aleatorios.\n",
        "- Cambiar el valor (por ejemplo, a np.random.seed(99)) generaría resultados distintos\n",
        "\n",
        "**for token in vocab:**\n",
        "Itera sobre cada token en el vocabulario.\n",
        "\n",
        "**np.random.rand(embedding_dim)**\n",
        "Genera un arreglo de números aleatorios entre 0 y 1 con tamaño igual a embedding_dim\n",
        "\n",
        "**¿Qué pasa si cambias algo?**\n",
        "- embedding_dim=100 → Genera vectores más largos con mayor capacidad de representación.\n",
        "- np.random.seed() → Un valor diferente cambia los números aleatorios"
      ],
      "metadata": {
        "id": "1bEmMaB3I3h8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutamos\n",
        "\n",
        "- Se define una lista de oraciones de ejemplo\n",
        "- Se aplican sucesivamente las funciones de limpieza, tokenización, división en conjuntos y generación del vocabulario\n",
        "- Se generan embeddings\n",
        "- See imprimen los resultados en consola para visualizar cada paso del pre-procesamiento"
      ],
      "metadata": {
        "id": "8iSpkLq1I38b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de datos: lista de oraciones en español\n",
        "textos = [\n",
        "        \"¡Hola! Chicos y chicas este es un ejemplo de pre-procesamiento de texto\",\n",
        "        \"El pre-procesamiento incluye la limpieza, normalización, tokenización, y generacion de embeddings\",\n",
        "        \"También se puede realizar este ejercicio pero usando librerias\",\n",
        "        \"Espero este ejercicio les sea de mucha utilidad para comprender el prerpocesamiento.\",\n",
        "        \"La creación de embeddings es opcional, pero muy útil para NLP.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "tuTL6ZZLI6PW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se creo un arreglo con oraciones"
      ],
      "metadata": {
        "id": "bE_AJ2RqJZYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpieza y Normalización\n",
        "textos_limpiados = [limpiar_texto(texto) for texto in textos]"
      ],
      "metadata": {
        "id": "619fY398Jbu0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuto la funcion **limpiar_texto** y le paso los textos uno por uno a la funcion gracias a la iteracion, estos textos limpios los almaceno en la variable t**extos_limpiados**"
      ],
      "metadata": {
        "id": "ZvE14DK4Ja8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización\n",
        "textos_tokenizados = [tokenizar_texto(texto) for texto in textos_limpiados]"
      ],
      "metadata": {
        "id": "YGkqV5lwJgcz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuto la funcion **tokenizar_texto** y le paso los textos ya limpiados uno por uno a la funcion gracias a la iteracion, estos textos tokenizados los almaceno en la variable **ttextos_tokenizados**"
      ],
      "metadata": {
        "id": "VZFBg1OBJlmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separación en conjuntos de datos\n",
        "# Utilizamos train_test_split para dividir el dataset en entrenamiento, validación y prueba\n",
        "# En este ejemplo, el conjunto de datos es pequeño; en aplicaciones reales se trabaja con muchos datos\n",
        "train_texts, temp_texts = train_test_split(textos_tokenizados, test_size=0.4, random_state=42)\n",
        "val_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "W4kYZu7lJl4Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train_texts, temp_texts = train_test_split(textos_tokenizados, test_size=0.4, random_state=42)**\n",
        "\n",
        "Usa la función train_test_split de sklearn.model_selection para dividir los datos en:\n",
        "\n",
        "- Conjunto de entrenamiento (train_texts)\n",
        "- Conjunto temporal (temp_texts) que será dividido más adelante en validación y prueba\n",
        "\n",
        "**Parámetros:**\n",
        "- textos_tokenizados → Lista de textos ya tokenizados (listas de tokens).\n",
        "- test_size=0.4 → El 40% de los datos se asigna al conjunto temporal -(temp_texts) y el 60% al conjunto de entrenamiento (train_texts)\n",
        "- random_state=42 → Fija la semilla aleatoria para asegurar resultados reproducibles.\n",
        "(Si cambias el valor, obtendrás una división diferente)\n",
        "\n",
        "**val_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)**\n",
        "\n",
        "Divide el conjunto temporal (temp_texts) en:\n",
        "\n",
        "- Conjunto de validación (val_texts)\n",
        "- Conjunto de prueba (test_texts)\n",
        "\n",
        "Parámetros:\n",
        "- temp_texts → Conjunto temporal generado en la división anterior.\n",
        "- test_size=0.5 → Divide a la mitad el conjunto temporal:\n",
        "  - 50% va al conjunto de validación.\n",
        "  - 50% al conjunto de prueba.\n",
        "- random_state=42 → Asegura la reproducibilidad.\n",
        "\n"
      ],
      "metadata": {
        "id": "eNrassV0J4ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generación del vocabulario (usando solo el conjunto de entrenamiento)\n",
        "vocab = generar_vocabulario(train_texts, min_freq=1)"
      ],
      "metadata": {
        "id": "ExCPm5A8J41w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecuto la funcion **generar_vocabulario** pra crear el vocabulario de acuerdo al set de datos de entrenemiento, este vocabulario lo guardo en la varibale **vocab**"
      ],
      "metadata": {
        "id": "5u7OSwByJ8av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de embeddings (opcional)\n",
        "embeddings = crear_embeddings(vocab, embedding_dim=50)"
      ],
      "metadata": {
        "id": "gnMuF7sjJ9Ee"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejectuto la funcion de **crear_embeddings** a la cual le paso el vocabulario generado para que construya y cree los embeddings de cada palabra del vocabulario en un tamaño en especifico en este caso en un embedding de 50 dimensiones y guardo los embedding creados en la varibale **embeddings**"
      ],
      "metadata": {
        "id": "eW9QhLyZKAlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "MjEFL60WKEzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Textos Originales:\")\n",
        "for t in textos:\n",
        "  print(\"  \", t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUK4xdFYKCj5",
        "outputId": "6373434b-d510-42f2-c98c-a79e214ac5c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos Originales:\n",
            "   ¡Hola! Chicos y chicas este es un ejemplo de pre-procesamiento de texto\n",
            "   El pre-procesamiento incluye la limpieza, normalización, tokenización, y generacion de embeddings\n",
            "   También se puede realizar este ejercicio pero usando librerias\n",
            "   Espero este ejercicio les sea de mucha utilidad para comprender el prerpocesamiento.\n",
            "   La creación de embeddings es opcional, pero muy útil para NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTextos Limpios y Normalizados:\")\n",
        "for t in textos_limpiados:\n",
        "    print(\"  \", t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaMU4Zq1KKVD",
        "outputId": "368c1a20-ca1b-453e-df27-6b1fe168d662"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Textos Limpios y Normalizados:\n",
            "   ¡hola chicos y chicas este es un ejemplo de preprocesamiento de texto\n",
            "   el preprocesamiento incluye la limpieza normalización tokenización y generacion de embeddings\n",
            "   también se puede realizar este ejercicio pero usando librerias\n",
            "   espero este ejercicio les sea de mucha utilidad para comprender el prerpocesamiento\n",
            "   la creación de embeddings es opcional pero muy útil para nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTokenización:\")\n",
        "for tokens in textos_tokenizados:\n",
        "    print(\"  \", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_2bR4KqKOIi",
        "outputId": "bf5fcc40-067a-4b18-c1f5-d3fbc60bb956"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenización:\n",
            "   ['¡hola', 'chicos', 'y', 'chicas', 'este', 'es', 'un', 'ejemplo', 'de', 'preprocesamiento', 'de', 'texto']\n",
            "   ['el', 'preprocesamiento', 'incluye', 'la', 'limpieza', 'normalización', 'tokenización', 'y', 'generacion', 'de', 'embeddings']\n",
            "   ['también', 'se', 'puede', 'realizar', 'este', 'ejercicio', 'pero', 'usando', 'librerias']\n",
            "   ['espero', 'este', 'ejercicio', 'les', 'sea', 'de', 'mucha', 'utilidad', 'para', 'comprender', 'el', 'prerpocesamiento']\n",
            "   ['la', 'creación', 'de', 'embeddings', 'es', 'opcional', 'pero', 'muy', 'útil', 'para', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVocabulario Generado:\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXYOglBLKRv5",
        "outputId": "88ddf906-b931-4bdd-c8ae-846b215e7918"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulario Generado:\n",
            "{'<PAD>': 0, '<UNK>': 1, 'también': 2, 'se': 3, 'puede': 4, 'realizar': 5, 'este': 6, 'ejercicio': 7, 'pero': 8, 'usando': 9, 'librerias': 10, '¡hola': 11, 'chicos': 12, 'y': 13, 'chicas': 14, 'es': 15, 'un': 16, 'ejemplo': 17, 'de': 18, 'preprocesamiento': 19, 'texto': 20, 'espero': 21, 'les': 22, 'sea': 23, 'mucha': 24, 'utilidad': 25, 'para': 26, 'comprender': 27, 'el': 28, 'prerpocesamiento': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: mostrar el embedding de la palabra \"preprocesamiento\"\n",
        "print(\"\\nEmbedding para la palabra 'preprocesamiento':\")\n",
        "# Notar que en nuestro proceso de limpieza no se conserva la tilde o signo,\n",
        "# y la palabra se tokeniza como 'preprocesamiento' (sin guion ni tilde)\n",
        "if \"preprocesamiento\" in embeddings:\n",
        "      print(embeddings[\"preprocesamiento\"])\n",
        "else:\n",
        "      print(\"  El token 'preprocesamiento' no se encuentra en el vocabulario.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL3L7e9wKUZk",
        "outputId": "30e99a20-9a15-4bd3-9838-5c6dab7ff4d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding para la palabra 'puede:\n",
            "[0.64203165 0.08413996 0.16162871 0.89855419 0.60642906 0.00919705\n",
            " 0.10147154 0.66350177 0.00506158 0.16080805 0.54873379 0.6918952\n",
            " 0.65196126 0.22426931 0.71217922 0.23724909 0.3253997  0.74649141\n",
            " 0.6496329  0.84922341 0.65761289 0.5683086  0.09367477 0.3677158\n",
            " 0.26520237 0.24398964 0.97301055 0.39309772 0.89204656 0.63113863\n",
            " 0.7948113  0.50263709 0.57690388 0.49251769 0.19524299 0.72245212\n",
            " 0.28077236 0.02431597 0.6454723  0.17711068 0.94045858 0.95392858\n",
            " 0.91486439 0.3701587  0.01545662 0.92831856 0.42818415 0.96665482\n",
            " 0.96361998 0.85300946]\n"
          ]
        }
      ]
    }
  ]
}