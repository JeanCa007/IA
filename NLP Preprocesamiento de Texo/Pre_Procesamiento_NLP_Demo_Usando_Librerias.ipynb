{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento de texto NLP USANDO LIBRERIAS**\n",
        "\n",
        "**By Jean Carlo Alvarez Ramirez**\n",
        "\n",
        "En la demo vamos a ver:\n",
        "\n",
        "- Limpieza, la remoción del contenido no deseado\n",
        "\n",
        "- Normalización, la conversión diferentes formas a una sola\n",
        "\n",
        "- Tokenización, la separación del texto en tókenes (unidades mínimas, por ejemplo palabras)\n",
        "\n",
        "- Separación en conjuntos de datos: entrenamiento, validación, prueba\n",
        "\n",
        "- Generación del vocabulario, la lista de tókenes conocidos\n",
        "\n",
        "- Creacion de embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "g4dS1hP7CHGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importar librerias"
      ],
      "metadata": {
        "id": "L-NyrwZRCqy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TWnoZkxACCWN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import re:** Importa el módulo re para trabajar con expresiones regulares, útil para buscar, reemplazar o filtrar texto.\n",
        "\n",
        "**import string:** Importa el módulo string que proporciona constantes y funciones para manipular cadenas, como listas de caracteres especiales o letras.\n",
        "\n",
        "**import random:** Importa el módulo random para generar números aleatorios, mezclar listas o realizar selecciones aleatorias.\n",
        "\n",
        "**import numpy as np:** Importa NumPy como np, una biblioteca para operaciones matemáticas y manipulación de arreglos multidimensionales.\n",
        "\n",
        "**from sklearn.model_selection import train_test_split:** Importa train_test_split de scikit-learn, que divide los datos en conjuntos de entrenamiento y prueba.\n",
        "\n",
        "**Parámetros importantes:**\n",
        "- test_size: Proporción de datos para el conjunto de prueba.\n",
        "- random_state: Fija la semilla para reproducibilidad.\n",
        "\n",
        "**import nltk:** Importa NLTK (Natural Language Toolkit), una biblioteca para procesamiento de lenguaje natural.\n",
        "\n",
        "**from nltk.tokenize import word_tokenize**: Importa word_tokenize para dividir texto en palabras (tokens).\n",
        "\n",
        "**from nltk.stem import WordNetLemmatizer:** Importa WordNetLemmatizer para reducir palabras a su forma base (ej: \"running\" → \"run\").\n",
        "\n",
        "**from nltk import pos_tag:** Importa pos_tag para etiquetar palabras con su categoría gramatical (sustantivo, verbo, etc.).\n",
        "\n",
        "**from nltk.corpus import movie_reviews:** Importa el corpus movie_reviews de NLTK, que contiene reseñas de películas etiquetadas como positivas o negativas.\n",
        "\n",
        "**from gensim.models import Word2Vec:** Importa Word2Vec de la biblioteca Gensim para crear modelos de vectores de palabras basados en su contexto.\n",
        "\n",
        "**Parámetros clave:**\n",
        "- vector_size: Tamaño del vector de cada palabra.\n",
        "- window: Número de palabras de contexto.\n",
        "- min_count: Palabras con frecuencia menor se descartan.\n",
        "- sg: Si es 1 usa Skip-Gram, si es 0 usa CBOW.\n",
        "\n",
        "**Parámetro sg en Word2Vec:**\n",
        "El parámetro sg en el modelo Word2Vec determina qué arquitectura utilizar para entrenar los vectores de palabras:\n",
        "\n",
        "- sg=0 → CBOW (Continuous Bag of Words)\n",
        "- sg=1 → Skip-Gram\n",
        "**¿Qué es CBOW (Continuous Bag of Words)?**\n",
        "\n",
        "Funcionamiento: Predice la palabra objetivo usando las palabras de contexto cercanas\n",
        "\n",
        "Ejemplo: Si tienes la frase:\n",
        "\n",
        "\"El gato está sobre la mesa\",\n",
        "y la palabra objetivo es \"está\", el modelo usa las palabras cercanas (\"El\", \"gato\", \"sobre\", \"la\") para predecir \"está\"\n",
        "\n",
        "Uso recomendado:\n",
        "- Con datasets grandes\n",
        "- Cuando se quiere mayor rapidez en el entrenamiento\n",
        "- Mejora la precisión en palabras frecuentes\n",
        "\n",
        "**¿Qué es Skip-Gram?**\n",
        "\n",
        "Funcionamiento: Hace lo contrario a CBOW. Usa la palabra objetivo para predecir su contexto\n",
        "\n",
        "Ejemplo: Con la misma frase y palabra objetivo \"está\", el modelo intenta predecir las palabras cercanas (\"El\", \"gato\", \"sobre\", \"la\")\n",
        "\n",
        "Uso recomendado:\n",
        "- Con datasets pequeños.\n",
        "- Mejora el rendimiento con palabras raras o poco frecuentes\n",
        "- Captura mejor las relaciones semánticas complejas\n",
        "\n",
        "- sg=0 (CBOW): El modelo entrenará más rápido y funcionará mejor en corpus grandes y palabras comunes\n",
        "\n",
        "- sg=1 (Skip-Gram): El entrenamiento será más lento, pero detectará mejor relaciones entre palabras raras o específicas\n"
      ],
      "metadata": {
        "id": "D3j4axWmCvwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargar recursos necesarios de NLTK"
      ],
      "metadata": {
        "id": "Bp7VKhjfCwiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBQfisgBCwLV",
        "outputId": "eaac6785-37a5-4e14-d53f-eb085b3b358d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nltk.download('punkt')**\n",
        "\n",
        "¿Qué hace? Descarga el tokenizador Punkt, que segmenta texto en oraciones y palabras\n",
        "\n",
        "Uso: Necesario para funciones como word_tokenize()\n",
        "\n",
        "**nltk.download('wordnet')**\n",
        "\n",
        "¿Qué hace? Descarga la base de datos WordNet, una red semántica de palabras en inglés\n",
        "\n",
        "Uso: Esencial para lematización con WordNetLemmatizer() y obtener sinónimos, antónimos o jerarquías léxicas\n",
        "\n",
        "**nltk.download('omw-1.4')**\n",
        "\n",
        "¿Qué hace? Descarga Open Multilingual WordNet, que amplía WordNet con traducciones a varios idiomas\n",
        "\n",
        "Uso: Permite lematización y análisis semántico en otros idiomas además del inglés\n",
        "\n",
        "**nltk.download('averaged_perceptron_tagger')**\n",
        "\n",
        "¿Qué hace? Descarga el etiquetador Averaged Perceptron Tagger, usado para análisis gramatical (Part-of-Speech tagging)\n",
        "\n",
        "Uso: Funciona con pos_tag() para asignar etiquetas gramaticales (sustantivo, verbo, adjetivo, etc.) a las palabras\n",
        "\n",
        "**nltk.download('movie_reviews')**\n",
        "\n",
        "¿Qué hace? Descarga el corpus de reseñas de películas de NLTK, con reseñas etiquetadas como positivas o negativas\n",
        "\n",
        "Uso: Popular para entrenar y probar modelos de análisis de sentimientos"
      ],
      "metadata": {
        "id": "GWpMaU8sC2Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar Dataset\n",
        "\n",
        "Cargamos el dataset usando el corpus de reseñas de películas\n",
        "\n",
        "La función cargar_dataset obtiene un número limitado de reseñas del corpus movie_reviews de NLTK. En este ejemplo se usan 10 reseñas para simplificar la demostración"
      ],
      "metadata": {
        "id": "9azxd1y2C2oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_dataset(num_reviews=10):\n",
        "    \"\"\"\n",
        "    Carga un número limitado de reseñas para el ejemplo.\n",
        "    \"\"\"\n",
        "    fileids = movie_reviews.fileids()[:num_reviews]\n",
        "    textos = [movie_reviews.raw(fileid) for fileid in fileids]\n",
        "    return textos"
      ],
      "metadata": {
        "id": "qzR8tvs_C6ti"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def cargar_dataset(num_reviews=10):**\n",
        "\n",
        "Define la función cargar_dataset con un parámetro num_reviews, que por defecto es 10\n",
        "\n",
        "- num_reviews controla cuántas reseñas se cargan\n",
        "\n",
        "**fileids = movie_reviews.fileids()[:num_reviews]**\n",
        "\n",
        "- movie_reviews.fileids() obtiene todos los identificadores de archivos del corpus movie_reviews\n",
        "- [:num_reviews] selecciona los primeros num_reviews identificadores\n",
        "\n",
        "**textos = [movie_reviews.raw(fileid) for fileid in fileids]**\n",
        "\n",
        "Usa una lista para iterar sobre los fileids\n",
        "\n",
        "- movie_reviews.raw(fileid) carga el texto completo de cada reseña\n",
        "\n",
        "Crea una lista llamada textos que contiene los textos cargados.\n"
      ],
      "metadata": {
        "id": "uQqZnId5C7PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpieza del texto\n",
        "\n",
        "Conversión a minúsculas, eliminación de dígitos y puntuación\n",
        "\n",
        "La función limpiar_texto transforma el texto a minúsculas, elimina dígitos y signos de puntuación, y recorta espacios adicionales, dejando un texto listo para la tokenizanizar"
      ],
      "metadata": {
        "id": "oH5YVLXXC7oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "    texto = re.sub(r'[' + re.escape(string.punctuation) + ']', '', texto)\n",
        "    texto = texto.strip()\n",
        "    return texto"
      ],
      "metadata": {
        "id": "66Y_hSTTDBWF"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def limpiar_texto(texto):**\n",
        "\n",
        "Define la función limpiar_texto que toma como parámetro un string texto\n",
        "\n",
        "**texto = texto.lower()**\n",
        "\n",
        "Convierte todo el texto a minúsculas\n",
        "\n",
        "Ejemplo: \"Hello World!\" → \"hello world!\"\n",
        "\n",
        "**texto = re.sub(r'\\d+', '', texto)**\n",
        "\n",
        "Usa expresiones regulares para eliminar todos los números del texto\n",
        "\n",
        "- r'\\d+' busca secuencias de uno o más dígitos\n",
        "\n",
        "Ejemplo: \"Movie 123\" → \"Movie \"\n",
        "\n",
        "**texto = re.sub(r'[' + re.escape(string.punctuation) + ']', '', texto)**\n",
        "\n",
        "Elimina los signos de puntuación utilizando **re.escape(string.punctuation)** para identificar caracteres especiales\n",
        "\n",
        "Ejemplo: \"Hello, world!\" → \"Hello world\"\n",
        "\n",
        "**texto = texto.strip()**\n",
        "\n",
        "Elimina espacios en blanco al inicio y al final del texto\n",
        "\n",
        "Ejemplo: \" hello world \" → \"hello world\"\n",
        "\n",
        "Que pasaria si cambiamos:\n",
        "\n",
        "- Eliminas texto.lower() → Mantendrás las mayúsculas\n",
        "- Cambias r'\\d+' por r'\\d' → Eliminarías cada dígito individualmente\n",
        "- Quitas re.escape(string.punctuation) → No eliminarías los signos de puntuación"
      ],
      "metadata": {
        "id": "YfEiFta7DCan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizacion\n",
        "\n",
        "Se utiliza word_tokenize de NLTK para separar el texto en tokens (palabras y otros símbolos)"
      ],
      "metadata": {
        "id": "QJqb2BEMDLhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizar_texto(texto):\n",
        "    \"\"\"\n",
        "    Separamos el texto en tokens usando word_tokenize de NLTK.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(texto)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "yB0zvsxyDQHM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def tokenizar_texto(texto):**\n",
        "\n",
        "Define la función tokenizar_texto que recibe un string texto como entrada\n",
        "\n",
        "**tokens = word_tokenize(texto)**\n",
        "\n",
        "Utiliza la función word_tokenize() de NLTK para dividir el texto en tokens (palabras, signos de puntuación, números, etc.)\n",
        "\n",
        "word_tokenize usa el modelo Punkt el cual  descargasmos con nltk.download('punkt')) en un inicio\n",
        "\n",
        "**Tener en cuenta?**\n",
        "- Incluye signos de puntuación: Si necesitas eliminarlos, debes filtrarlos después\n",
        "\n",
        "- Respeta espacios y caracteres especiales: Cada palabra y símbolo se considera un token separado\n",
        "\n",
        "- Soporta varios idiomas, aunque funciona mejor con inglés por defecto"
      ],
      "metadata": {
        "id": "fJh4DpHPDYR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizacion\n",
        "\n",
        "Lematización con POS-tagging\n",
        "\n",
        "La función get_wordnet_pos mapea las etiquetas de parte del habla (POS) generadas por pos_tag de NLTK al formato esperado por WordNetLemmatizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "zWZ2TP9oDYlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(nltk_pos_tag):\n",
        "    \"\"\"\n",
        "    Convierte la etiqueta POS de NLTK a un formato compatible con WordNetLemmatizer.\n",
        "    \"\"\"\n",
        "    if nltk_pos_tag.startswith('J'):\n",
        "        return 'a'  # adjetivo\n",
        "    elif nltk_pos_tag.startswith('V'):\n",
        "        return 'v'  # verbo\n",
        "    elif nltk_pos_tag.startswith('N'):\n",
        "        return 'n'  # sustantivo\n",
        "    elif nltk_pos_tag.startswith('R'):\n",
        "        return 'r'  # adverbio\n",
        "    else:\n",
        "        return 'n'  # por defecto, sustantivo"
      ],
      "metadata": {
        "id": "jdZ6LVWiDmIS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def get_wordnet_pos(nltk_pos_tag):**\n",
        "\n",
        "Define la función get_wordnet_pos que recibe nltk_pos_tag, la etiqueta gramatical generada por pos_tag() de NLTK\n",
        "\n",
        "**if nltk_pos_tag.startswith('J'):**\n",
        "\n",
        "Si la etiqueta empieza con 'J' (ej. 'JJ' para adjetivos), devuelve 'a' (adjetivo) para WordNet\n",
        "\n",
        "**elif nltk_pos_tag.startswith('V'):**\n",
        "\n",
        "Si la etiqueta empieza con 'V' (ej. 'VB', 'VBD', 'VBG'), devuelve 'v' (verbo)\n",
        "\n",
        "**elif nltk_pos_tag.startswith('N'):**\n",
        "\n",
        "Si la etiqueta empieza con 'N' (ej. 'NN' para sustantivos), devuelve 'n' (sustantivo)\n",
        "\n",
        "**elif nltk_pos_tag.startswith('R'):**\n",
        "\n",
        "Si la etiqueta empieza con 'R' (ej. 'RB' para adverbios), devuelve 'r' (adverbio)\n",
        "\n",
        "**else: return 'n'**\n",
        "\n",
        "Si la etiqueta no coincide con las anteriores, asigna por defecto 'n' (sustantivo)\n",
        "\n",
        "**Por qué es necesaria esta función?**\n",
        "\n",
        "- WordNetLemmatizer necesita etiquetas específicas ('a', 'v', 'n', 'r') para realizar una lematización precisa\n",
        "\n",
        "- Las etiquetas de pos_tag() siguen el estándar Penn Treebank (ej.: 'JJ', 'VB', 'NN', 'RB'), por lo que se requiere esta conversión\n",
        "\n",
        "**Qué pasa si no uso esta función?**\n",
        "\n",
        "El lematizador asume que todas las palabras son sustantivos\n",
        "\n",
        "Ejemplo:\n",
        "- Sin conversión: \"running\" → \"running\"\n",
        "- Con conversión ('V'): \"running\" → \"run\"\n",
        "\n",
        "En WordNetLemmatizer se usan específicamente las etiquetas 'a', 'v', 'n' y 'r' porque son los tipos de palabras que WordNet reconoce para la lematización. Cualquier otra etiqueta no será interpretada correctamente por el lematizador.\n",
        "\n",
        "**Etiquetas válidas para WordNetLemmatizer:**\n",
        "\n",
        "**Etiqueta\tCategoría Gramatical**\n",
        "\n",
        "**Ejemplo**\n",
        "- 'n'\tNoun (Sustantivo)\tdog → dog\n",
        "\n",
        "- 'v'\tVerb (Verbo)\trunning → run\n",
        "\n",
        "- 'a'\tAdjective (Adjetivo)\tbetter → good\n",
        "\n",
        "- 'r'\tAdverb (Adverbio)\tquickly → quick\n",
        "\n",
        "- 's'\tAdjective Satellite (Raro)\tgood-looking → good\n",
        "\n",
        "Aunque 's' existe en WordNet, la mayoría de lematizadores lo tratan como 'a'\n",
        "\n",
        "**Qué pasa si uso otras letras?**\n",
        "\n",
        "Si pasas una letra no válida (ej. 'P' para pronombres o 'C' para conjunciones), WordNetLemmatizer no sabrá cómo tratarla\n",
        "\n",
        "En ese caso:\n",
        "- No se aplica la lematización y la palabra queda igual\n",
        "- O se toma la categoría por defecto ('n', sustantivo)."
      ],
      "metadata": {
        "id": "iS-SB-YIDqqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizar_tokens(tokens):\n",
        "    \"\"\"\n",
        "    Lematiza cada token utilizando su parte del habla obtenida con pos_tag\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tokens = pos_tag(tokens)\n",
        "    lemmatized_tokens = [\n",
        "        lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in pos_tokens\n",
        "    ]\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "3wgfF7iIDur4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalizar_tokens lematiza cada token utilizando el POS correspondiente, reduciendo las palabras a su forma base\n",
        "\n",
        "**def normalizar_tokens(tokens):**\n",
        "\n",
        "Define la función que toma como entrada una lista de tokens (palabras individuales)\n",
        "\n",
        "**lemmatizer = WordNetLemmatizer()**\n",
        "\n",
        "Crea una instancia del lematizador de WordNet\n",
        "\n",
        "Este se usa para reducir palabras a su forma base (lematización)\n",
        "\n",
        "**pos_tokens = pos_tag(tokens)**\n",
        "\n",
        "Usa pos_tag() de NLTK para asignar etiquetas gramaticales a cada token\n",
        "Ejemplo:\n",
        "- Codigo : pos_tag(['running', 'fast'])\n",
        "- Salida: [('running', 'VBG'), ('fast', 'RB')]\n",
        "\n",
        "**lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in pos_tokens]**\n",
        "\n",
        "Usa una list comprehension para lematizar cada token\n",
        "\n",
        "- get_wordnet_pos(tag) convierte la etiqueta de NLTK (ej. 'VBG') al formato compatible con WordNet (ej. 'v')\n",
        "\n",
        "- lemmatizer.lemmatize() aplica la lematización usando la categoría correcta\n",
        "\n",
        "**Qué hacemos exactamente en esta funcion?**\n",
        "\n",
        "Identifica el rol gramatical de cada palabra (sustantivo, verbo, adjetivo, adverbio)\n",
        "\n",
        "Lematiza basándose en ese rol:\n",
        "\n",
        "- \"hanging\" (verbo) → \"hang\"\n",
        "- \"feet\" (sustantivo) → \"foot\"\n",
        "- \"best\" (adjetivo) → \"good\"\n",
        "\n"
      ],
      "metadata": {
        "id": "7TmerltBDrD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gnerar Vocabulario\n",
        "\n",
        "Generamos vocabulario a partir de los tokens del conjunto de entrenamiento\n",
        "\n",
        "Se recorre el conjunto de entrenamiento para contar la frecuencia de cada token y asignar un identificador único. Se incluyen tokens especiales (PAD y UNK) para facilitar futuros procesos"
      ],
      "metadata": {
        "id": "PFUuOcYrDyVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_vocabulario(tokenized_texts, min_freq=1):\n",
        "    token_freq = {}\n",
        "    for tokens in tokenized_texts:\n",
        "        for token in tokens:\n",
        "            token_freq[token] = token_freq.get(token, 0) + 1\n",
        "\n",
        "    # Se agregan tokens especiales\n",
        "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    next_id = 2\n",
        "    for token, freq in token_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[token] = next_id\n",
        "            next_id += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "TgEzMBdxD516"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def generar_vocabulario(tokenized_texts, min_freq=1):**\n",
        "\n",
        "Define la función que genera un vocabulario a partir de textos tokenizados\n",
        "\n",
        "**Parámetros:**\n",
        "- tokenized_texts: Lista de listas de tokens\n",
        "- min_freq: Frecuencia mínima para incluir un token en el vocabulario (por defecto, 1)\n",
        "\n",
        "**token_freq = {}**\n",
        "\n",
        "Crea un diccionario vacío para contar la frecuencia de cada token\n",
        "\n",
        "**for tokens in tokenized_texts:**\n",
        "\n",
        "Itera sobre cada lista de tokens en los textos\n",
        "\n",
        "**for token in tokens:**\n",
        "\n",
        "Itera sobre cada token individual\n",
        "\n",
        "**token_freq[token] = token_freq.get(token, 0) + 1**\n",
        "\n",
        "Cuenta cuántas veces aparece cada token\n",
        "\n",
        "- get(token, 0) devuelve 0 si el token no está aún en el diccionario\n",
        "\n",
        "**vocab = {\"PAD\": 0, \"UNK\": 1}**\n",
        "\n",
        "Inicializa el vocabulario con dos tokens especiales:\n",
        "\n",
        "- PAD (ID: 0) → Para rellenar secuencias cortas hasta igualarlas en longitud\n",
        "\n",
        "- UNK (ID: 1) → Para palabras desconocidas o fuera de vocabulario\n",
        "\n",
        "**next_id = 2**\n",
        "\n",
        "Comienza a asignar IDs desde 2 para los tokens reales\n",
        "\n",
        "**for token, freq in token_freq.items():**\n",
        "\n",
        "Itera sobre los tokens y sus frecuencias\n",
        "\n",
        "**if freq >= min_freq:**\n",
        "\n",
        "Solo incluye tokens que aparecen al menos min_freq veces\n",
        "\n",
        "**vocab[token] = next_id**\n",
        "\n",
        "Asigna un ID único al token y lo agrega al vocabulario\n",
        "\n",
        "**next_id += 1**\n",
        "\n",
        "Incrementa el ID para el siguiente token\n",
        "\n",
        "**Que pasa si cambias min_freq?**\n",
        "- min_freq=1 → Incluye todos los tokens que aparecen al menos una vez\n",
        "- min_freq=2 → Solo incluye tokens que aparecen al menos dos veces\n",
        "\n",
        "**Por qué usar PAD y UNK?**\n",
        "- PAD : Para crear lotes de datos uniformes en tamaño al entrenar modelos\n",
        "- UNK : Maneja palabras no vistas durante el entrenamiento\n"
      ],
      "metadata": {
        "id": "ahxGgTXUD-IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creacion de Embeddings\n",
        "\n",
        "Usmos Word2Vec\n",
        "\n",
        "Se entrena un modelo Word2Vec con los textos de entrenamiento (ya tokenizados y lematizados) para generar representaciones vectoriales  de cada token. Para cada token del vocabulario, si se encuentra en el modelo, se asigna su vector; en caso contrario, se asigna un vector de ceros"
      ],
      "metadata": {
        "id": "YfJBK4ghEA8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_embeddings(train_texts, vocab, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Entrena un modelo Word2Vec con los textos de entrenamiento y genera una representación\n",
        "    vectorial para cada token del vocabulario\n",
        "    \"\"\"\n",
        "    # Entrenar Word2Vec usando los textos (tokenizados y lematizados)\n",
        "    w2v_model = Word2Vec(\n",
        "        sentences=train_texts,\n",
        "        vector_size=embedding_dim,\n",
        "        window=5,\n",
        "        min_count=1,\n",
        "        workers=4,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Crear un diccionario de embeddings: si el token no se encuentra, se asigna un vector de ceros.\n",
        "    embeddings = {}\n",
        "    for token in vocab.keys():\n",
        "        if token in w2v_model.wv:\n",
        "            embeddings[token] = w2v_model.wv[token]\n",
        "        else:\n",
        "            embeddings[token] = np.zeros(embedding_dim)\n",
        "    return embeddings, w2v_model"
      ],
      "metadata": {
        "id": "DGFqqRAsEKc_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**def crear_embeddings(train_texts, vocab, embedding_dim=50):**\n",
        "\n",
        "Define la función que genera vectores de palabras (embeddings) usando Word2Vec\n",
        "\n",
        "**Parámetros:**\n",
        "- train_texts: Lista de textos tokenizados para entrenar el modelo\n",
        "- vocab: Diccionario de vocabulario generado previamente\n",
        "- embedding_dim: Dimensión de los vectores de palabras (por defecto, 50)\n",
        "\n",
        "**Entrenamiento del modelo Word2Vec:**\n",
        "\n",
        "**w2v_model = Word2Vec(**\n",
        "\n",
        "Entrena un modelo Word2Vec con los textos proporcionados\n",
        "\n",
        "**Parámetros:**\n",
        "\n",
        "- sentences=train_texts: Los textos tokenizados usados para el entrenamiento\n",
        "- vector_size=embedding_dim: Dimensión de los vectores generados (50 por defecto)\n",
        "- window=5: Número de palabras de contexto consideradas antes y después de la palabra objetivo\n",
        "- min_count=1: Incluye todas las palabras que aparecen al menos 1 vez\n",
        "- workers=4: Usa 4 núcleos para paralelizar el entrenamiento\n",
        "- seed=42: Fija la semilla para resultados reproducibles\n",
        "\n",
        "**Generación del diccionario de embeddings:**\n",
        "\n",
        "**embeddings = {}**\n",
        "\n",
        "Crea un diccionario vacío para almacenar los vectores de cada token\n",
        "\n",
        "**for token in vocab.keys():**\n",
        "\n",
        "Itera sobre todos los tokens del vocabulario\n",
        "\n",
        "**if token in w2v_model.wv:**\n",
        "\n",
        "Verifica si el token tiene un vector entrenado en el modelo\n",
        "\n",
        "**embeddings[token] = w2v_model.wv[token]**\n",
        "\n",
        "Si el token está en el modelo, asigna su vector al diccionario\n",
        "\n",
        "**else: embeddings[token] = np.zeros(embedding_dim)**\n",
        "\n",
        "Si el token no tiene vector (poco frecuente o desconocido), asigna un vector de ceros\n",
        "\n",
        "**return embeddings, w2v_model**\n",
        "\n",
        "Devuelve:\n",
        "- embeddings : Diccionario de vectores para cada token.\n",
        "- w2v_model : El modelo entrenado (para futuros usos como similitud de palabras)\n",
        "\n",
        "**Que pasa si cambias los parámetros?**\n",
        "- embedding_dim: Aumentarlo puede capturar más información semántica, pero requiere más memoria\n",
        "\n",
        "- window: Valores grandes amplían el contexto, pero pueden diluir el enfoque en palabras cercanas\n",
        "\n",
        "- min_count: Si lo subes (ej. min_count=2), se ignorarán palabras poco frecuentes"
      ],
      "metadata": {
        "id": "KBJuSD44EO-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutamos el preprocesamiento"
      ],
      "metadata": {
        "id": "EhhBG-4MEY9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar dataset (se utilizan 10 reseñas para el ejemplo)\n",
        "textos = cargar_dataset(num_reviews=10)\n",
        "\n",
        "# Pre-procesamiento:\n",
        "# Limpieza del texto\n",
        "textos_limpiados = [limpiar_texto(texto) for texto in textos]\n",
        "\n",
        "# Tokenización utilizando NLTK\n",
        "tokenized_texts = [tokenizar_texto(texto) for texto in textos_limpiados]\n",
        "\n",
        "# Normalización: Lematización de los tokens\n",
        "normalized_texts = [normalizar_tokens(tokens) for tokens in tokenized_texts]\n",
        "\n",
        "# División en conjuntos: Entrenamiento, Validación y Prueba\n",
        "train_texts, temp_texts = train_test_split(normalized_texts, test_size=0.4, random_state=42)\n",
        "val_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)\n",
        "\n",
        "# Generación del vocabulario usando solo el conjunto de entrenamiento\n",
        "vocab = generar_vocabulario(train_texts, min_freq=1)\n",
        "\n",
        "# Creación de embeddings usando Word2Vec\n",
        "embeddings, w2v_model = crear_embeddings(train_texts, vocab, embedding_dim=50)"
      ],
      "metadata": {
        "id": "_kVHwerAEe8A"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos todas las funciones creadas"
      ],
      "metadata": {
        "id": "hiALbXPSEkRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "aNTR3YCGE0TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ejemplos de textos originales:\")\n",
        "for t in textos[:2]:\n",
        "    print(\"  \", t[:100], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VKytjRLE8wD",
        "outputId": "ee27c46e-ca42-4636-94a2-2b8d565e60d5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplos de textos originales:\n",
            "   plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            " ...\n",
            "   the happy bastard's quick movie review \n",
            "damn that y2k bug . \n",
            "it's got a head start in this movie sta ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTextos Limpiados, Tokenizados y Lematizados:\")\n",
        "for tokens in normalized_texts[:2]:\n",
        "    print(\"  \", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2njQnrWE-rw",
        "outputId": "2061d4b0-6e10-4505-ddd1-20fd55d654dd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Textos Limpiados, Tokenizados y Lematizados:\n",
            "   ['plot', 'two', 'teen', 'couple', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of', 'the', 'guys', 'dy', 'but', 'his', 'girlfriend', 'continue', 'to', 'see', 'him', 'in', 'her', 'life', 'and', 'have', 'nightmare', 'whats', 'the', 'deal', 'watch', 'the', 'movie', 'and', 'sorta', 'find', 'out', 'critique', 'a', 'mindfuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touch', 'on', 'a', 'very', 'cool', 'idea', 'but', 'present', 'it', 'in', 'a', 'very', 'bad', 'package', 'which', 'be', 'what', 'make', 'this', 'review', 'an', 'even', 'hard', 'one', 'to', 'write', 'since', 'i', 'generally', 'applaud', 'film', 'which', 'attempt', 'to', 'break', 'the', 'mold', 'mess', 'with', 'your', 'head', 'and', 'such', 'lose', 'highway', 'memento', 'but', 'there', 'be', 'good', 'and', 'bad', 'way', 'of', 'make', 'all', 'type', 'of', 'film', 'and', 'these', 'folk', 'just', 'didnt', 'snag', 'this', 'one', 'correctly', 'they', 'seem', 'to', 'have', 'take', 'this', 'pretty', 'neat', 'concept', 'but', 'execute', 'it', 'terribly', 'so', 'what', 'be', 'the', 'problem', 'with', 'the', 'movie', 'well', 'it', 'main', 'problem', 'be', 'that', 'it', 'simply', 'too', 'jumbled', 'it', 'start', 'off', 'normal', 'but', 'then', 'downshifts', 'into', 'this', 'fantasy', 'world', 'in', 'which', 'you', 'a', 'an', 'audience', 'member', 'have', 'no', 'idea', 'whats', 'go', 'on', 'there', 'be', 'dream', 'there', 'be', 'character', 'come', 'back', 'from', 'the', 'dead', 'there', 'be', 'others', 'who', 'look', 'like', 'the', 'dead', 'there', 'be', 'strange', 'apparition', 'there', 'be', 'disappearance', 'there', 'be', 'a', 'looooot', 'of', 'chase', 'scene', 'there', 'be', 'ton', 'of', 'weird', 'thing', 'that', 'happen', 'and', 'most', 'of', 'it', 'be', 'simply', 'not', 'explain', 'now', 'i', 'personally', 'dont', 'mind', 'try', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', 'but', 'when', 'all', 'it', 'do', 'be', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', 'i', 'get', 'kind', 'of', 'feed', 'up', 'after', 'a', 'while', 'which', 'be', 'this', 'films', 'big', 'problem', 'it', 'obviously', 'get', 'this', 'big', 'secret', 'to', 'hide', 'but', 'it', 'seem', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'it', 'final', 'five', 'minute', 'and', 'do', 'they', 'make', 'thing', 'entertain', 'thrilling', 'or', 'even', 'engage', 'in', 'the', 'meantime', 'not', 'really', 'the', 'sad', 'part', 'be', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flick', 'like', 'this', 'so', 'we', 'actually', 'figure', 'most', 'of', 'it', 'out', 'by', 'the', 'halfway', 'point', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'do', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', 'but', 'it', 'still', 'didnt', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movie', 'like', 'this', 'be', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'be', 'into', 'it', 'even', 'before', 'they', 'be', 'give', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understand', 'i', 'mean', 'show', 'melissa', 'sagemiller', 'run', 'away', 'from', 'vision', 'for', 'about', 'minute', 'throughout', 'the', 'movie', 'be', 'just', 'plain', 'lazy', 'okay', 'we', 'get', 'it', 'there', 'be', 'people', 'chase', 'her', 'and', 'we', 'dont', 'know', 'who', 'they', 'be', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', 'how', 'about', 'give', 'u', 'different', 'scene', 'offer', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'go', 'down', 'in', 'the', 'movie', 'apparently', 'the', 'studio', 'take', 'this', 'film', 'away', 'from', 'it', 'director', 'and', 'chop', 'it', 'up', 'themselves', 'and', 'it', 'show', 'there', 'mightve', 'be', 'a', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'in', 'here', 'somewhere', 'but', 'i', 'guess', 'the', 'suit', 'decide', 'that', 'turn', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', 'would', 'make', 'more', 'sense', 'the', 'actor', 'be', 'pretty', 'good', 'for', 'the', 'most', 'part', 'although', 'wes', 'bentley', 'just', 'seem', 'to', 'be', 'play', 'the', 'exact', 'same', 'character', 'that', 'he', 'do', 'in', 'american', 'beauty', 'only', 'in', 'a', 'new', 'neighborhood', 'but', 'my', 'big', 'kudos', 'go', 'out', 'to', 'sagemiller', 'who', 'hold', 'her', 'own', 'throughout', 'the', 'entire', 'film', 'and', 'actually', 'have', 'you', 'feel', 'her', 'character', 'unravel', 'overall', 'the', 'film', 'doesnt', 'stick', 'because', 'it', 'doesnt', 'entertain', 'it', 'confuse', 'it', 'rarely', 'excite', 'and', 'it', 'feel', 'pretty', 'redundant', 'for', 'most', 'of', 'it', 'runtime', 'despite', 'a', 'pretty', 'cool', 'end', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'come', 'before', 'it', 'oh', 'and', 'by', 'the', 'way', 'this', 'be', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', 'it', 'just', 'package', 'to', 'look', 'that', 'way', 'because', 'someone', 'be', 'apparently', 'assume', 'that', 'the', 'genre', 'be', 'still', 'hot', 'with', 'the', 'kid', 'it', 'also', 'wrap', 'production', 'two', 'year', 'ago', 'and', 'have', 'be', 'sit', 'on', 'the', 'shelf', 'ever', 'since', 'whatever', 'skip', 'it', 'wheres', 'joblo', 'come', 'from', 'a', 'nightmare', 'of', 'elm', 'street', 'blair', 'witch', 'the', 'crow', 'the', 'crow', 'salvation', 'lose', 'highway', 'memento', 'the', 'others', 'stir', 'of', 'echo']\n",
            "   ['the', 'happy', 'bastard', 'quick', 'movie', 'review', 'damn', 'that', 'yk', 'bug', 'it', 'get', 'a', 'head', 'start', 'in', 'this', 'movie', 'star', 'jamie', 'lee', 'curtis', 'and', 'another', 'baldwin', 'brother', 'william', 'this', 'time', 'in', 'a', 'story', 'regard', 'a', 'crew', 'of', 'a', 'tugboat', 'that', 'come', 'across', 'a', 'deserted', 'russian', 'tech', 'ship', 'that', 'have', 'a', 'strangeness', 'to', 'it', 'when', 'they', 'kick', 'the', 'power', 'back', 'on', 'little', 'do', 'they', 'know', 'the', 'power', 'within', 'go', 'for', 'the', 'gore', 'and', 'bring', 'on', 'a', 'few', 'action', 'sequence', 'here', 'and', 'there', 'virus', 'still', 'feel', 'very', 'empty', 'like', 'a', 'movie', 'go', 'for', 'all', 'flash', 'and', 'no', 'substance', 'we', 'dont', 'know', 'why', 'the', 'crew', 'be', 'really', 'out', 'in', 'the', 'middle', 'of', 'nowhere', 'we', 'dont', 'know', 'the', 'origin', 'of', 'what', 'take', 'over', 'the', 'ship', 'just', 'that', 'a', 'big', 'pink', 'flashy', 'thing', 'hit', 'the', 'mir', 'and', 'of', 'course', 'we', 'dont', 'know', 'why', 'donald', 'sutherland', 'be', 'stumble', 'around', 'drunkenly', 'throughout', 'here', 'it', 'just', 'hey', 'let', 'chase', 'these', 'people', 'around', 'with', 'some', 'robot', 'the', 'acting', 'be', 'below', 'average', 'even', 'from', 'the', 'like', 'of', 'curtis', 'youre', 'more', 'likely', 'to', 'get', 'a', 'kick', 'out', 'of', 'her', 'work', 'in', 'halloween', 'h', 'sutherland', 'be', 'waste', 'and', 'baldwin', 'well', 'he', 'act', 'like', 'a', 'baldwin', 'of', 'course', 'the', 'real', 'star', 'here', 'be', 'stan', 'winstons', 'robot', 'design', 'some', 'schnazzy', 'cgi', 'and', 'the', 'occasional', 'good', 'gore', 'shot', 'like', 'pick', 'into', 'someone', 'brain', 'so', 'if', 'robot', 'and', 'body', 'part', 'really', 'turn', 'you', 'on', 'here', 'your', 'movie', 'otherwise', 'it', 'pretty', 'much', 'a', 'sunken', 'ship', 'of', 'a', 'movie']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVocabulario Generado:\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbylnhPaFBWA",
        "outputId": "adc7c4e2-fafc-45b2-ac0c-c15aa66bb739"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulario Generado:\n",
            "{'<PAD>': 0, '<UNK>': 1, 'thats': 2, 'exactly': 3, 'how': 4, 'long': 5, 'the': 6, 'movie': 7, 'felt': 8, 'to': 9, 'me': 10, 'there': 11, 'werent': 12, 'even': 13, 'nine': 14, 'laugh': 15, 'in': 16, 'month': 17, 'it': 18, 'a': 19, 'terrible': 20, 'mess': 21, 'of': 22, 'star': 23, 'man': 24, 'mr': 25, 'hugh': 26, 'grant': 27, 'huge': 28, 'dork': 29, 'not': 30, 'whole': 31, 'oralsexprostitution': 32, 'thing': 33, 'refer': 34, 'that': 35, 'bug': 36, 'fact': 37, 'be': 38, 'annoy': 39, 'just': 40, 'adam': 41, 'sandlerannoying': 42, 'talk': 43, 'jim': 44, 'carreyannoying': 45, 'since': 46, 'when': 47, 'do': 48, 'eye': 49, 'flutter': 50, 'and': 51, 'nervous': 52, 'smile': 53, 'pas': 54, 'for': 55, 'act': 56, 'but': 57, 'on': 58, 'other': 59, 'hand': 60, 'really': 61, 'bad': 62, 'slapstick': 63, 'fistfight': 64, 'delivery': 65, 'room': 66, 'culminating': 67, 'head': 68, 'joan': 69, 'cusacks': 70, 'lapa': 71, 'scene': 72, 'he': 73, 'pay': 74, 'have': 75, 'include': 76, 'obscene': 77, 'double': 78, 'entendres': 79, 'robin': 80, 'williams': 81, 'obstetrician': 82, 'tell': 83, 'pregnant': 84, 'girlfriend': 85, 'she': 86, 'big': 87, 'pussy': 88, 'referring': 89, 'course': 90, 'size': 91, 'cat': 92, 'hair': 93, 'her': 94, 'coat': 95, 'nonetheless': 96, 'exchange': 97, 'comedy': 98, 'predictable': 99, 'cookiecutter': 100, 'with': 101, 'no': 102, 'originality': 103, 'humor': 104, 'or': 105, 'plot': 106, 'play': 107, 'successful': 108, 'child': 109, 'psychiatrist': 110, 'why': 111, 'psychologist': 112, 'so': 113, 'scriptwriter': 114, 'could': 115, 'inject': 116, 'follow': 117, 'unfunny': 118, 'kid': 119, 'my': 120, 'dad': 121, 'an': 122, 'asshole': 123, 'eyelashes': 124, 'offer': 125, 'then': 126, 'respond': 127, 'his': 128, 'english': 129, 'accent': 130, 'ithinkiactuallyhave': 131, 'talent': 132, 'attitude': 133, 'you': 134, 'possibly': 135, 'elaborate': 136, 'dads': 137, 'more': 138, 'like': 139, 'beside': 140, 'point': 141, 'which': 142, 'too': 143, 'many': 144, 'needlessly': 145, 'stupid': 146, 'joke': 147, 'get': 148, 'from': 149, 'ten': 150, 'year': 151, 'old': 152, 'audience': 153, 'while': 154, 'everyone': 155, 'else': 156, 'shake': 157, 'disbelief': 158, 'anyway': 159, 'find': 160, 'out': 161, 'usual': 162, 'reaction': 163, 'eyelash': 164, 'this': 165, 'pave': 166, 'way': 167, 'every': 168, 'possible': 169, 'pregnancychild': 170, 'birth': 171, 'gag': 172, 'book': 173, 'especially': 174, 'equally': 175, 'friend': 176, 'wife': 177, 'also': 178, 'by': 179, 'tom': 180, 'arnold': 181, 'who': 182, 'provide': 183, 'most': 184, 'cacophonous': 185, 'none': 186, 'funny': 187, 'such': 188, 'where': 189, 'beat': 190, 'up': 191, 'costume': 192, 'arnie': 193, 'dinosaur': 194, 'draw': 195, 'your': 196, 'own': 197, 'parallel': 198, 'one': 199, 'toy': 200, 'store': 201, 'only': 202, 'interesting': 203, 'character': 204, 'jeff': 205, 'goldblum': 206, 'should': 207, 'hide': 208, 'himself': 209, 'away': 210, 'somewhere': 211, 'after': 212, 'dreadful': 213, 'hideaway': 214, 'artist': 215, 'fear': 216, 'simultaneous': 217, 'longing': 218, 'commitment': 219, 'russian': 220, 'doctor': 221, 'recently': 222, 'decide': 223, 'switch': 224, 'veterinary': 225, 'medicine': 226, 'obstetrics': 227, 'much': 228, 'onejoke': 229, 'foreignguywhomispronouncesenglish': 230, 'stereotype': 231, 'someone': 232, 'say': 233, 'yakov': 234, 'smirnov': 235, 'favorite': 236, 'vodka': 237, 'hence': 238, 'line': 239, 'now': 240, 'time': 241, 'take': 242, 'look': 243, 'at': 244, 'volvo': 245, 'another': 246, 'nasty': 247, 'unamusing': 248, 'except': 249, 'go': 250, 'right': 251, 'over': 252, 'adult': 253, 'simultaneously': 254, 'groan': 255, 'complete': 256, 'failure': 257, 'low': 258, 'intelligence': 259, 'high': 260, 'loud': 261, 'fail': 262, 'uninspired': 263, 'lunacy': 264, 'sunset': 265, 'boulevard': 266, 'arrest': 267, 'please': 268, 'caughtwithhispantsdown': 269, 'may': 270, 'bring': 271, 'people': 272, 'into': 273, 'theater': 274, 'they': 275, 'certainly': 276, 'wont': 277, 'leave': 278, 'their': 279, 'face': 280, 'minute': 281, 'everything': 282, 'force': 283, 'unauthentic': 284, 'anyone': 285, 'i': 286, 'q': 287, 'sorry': 288, 'will': 289, 'know': 290, 'waste': 291, 'money': 292, 'unfulfilled': 293, 'desire': 294, 'least': 295, 'didnt': 296, 'spend': 297, 'buck': 298, 'these': 299, 'make': 300, 'jaded': 301, 'viewer': 302, 'thankful': 303, 'invention': 304, 'timex': 305, 'indiglo': 306, 'watch': 307, 'base': 308, 'late': 309, 's': 310, 'television': 311, 'show': 312, 'same': 313, 'name': 314, 'mod': 315, 'squad': 316, 'tale': 317, 'three': 318, 'reformed': 319, 'criminal': 320, 'under': 321, 'employ': 322, 'police': 323, 'undercover': 324, 'however': 325, 'wrong': 326, 'evidence': 327, 'steal': 328, 'immediately': 329, 'suspicion': 330, 'ad': 331, 'seem': 332, 'quick': 333, 'cut': 334, 'cool': 335, 'music': 336, 'claire': 337, 'dane': 338, 'nice': 339, 'cute': 340, 'outfit': 341, 'car': 342, 'chase': 343, 'stuff': 344, 'blow': 345, 'sound': 346, 'first': 347, 'fifteen': 348, 'quickly': 349, 'become': 350, 'apparent': 351, 'slick': 352, 'production': 353, 'simply': 354, 'isnt': 355, 'enough': 356, 'film': 357, 'best': 358, 'describe': 359, 'cross': 360, 'between': 361, 'hourlong': 362, 'cop': 363, 'video': 364, 'both': 365, 'stretch': 366, 'span': 367, 'hour': 368, 'half': 369, 'come': 370, 'single': 371, 'clich': 372, 'doesnt': 373, 'matter': 374, 'element': 375, 'recycle': 376, 'weve': 377, 'already': 378, 'see': 379, 'nothing': 380, 'spectacular': 381, 'sometimes': 382, 'border': 383, 'wooden': 384, 'omar': 385, 'epps': 386, 'deliver': 387, 'if': 388, 'bore': 389, 'transfer': 390, 'onto': 391, 'escape': 392, 'relatively': 393, 'unscathed': 394, 'giovanni': 395, 'ribisi': 396, 'resident': 397, 'crazy': 398, 'ultimately': 399, 'worth': 400, 'unfortunately': 401, 'hes': 402, 'save': 403, 'convoluted': 404, 'all': 405, 'dont': 406, 'apart': 407, 'occupy': 408, 'screen': 409, 'young': 410, 'cast': 411, 'clothes': 412, 'hip': 413, 'soundtrack': 414, 'appear': 415, 'gear': 416, 'towards': 417, 'teenage': 418, 'mindset': 419, 'despite': 420, 'american': 421, 'r': 422, 'rating': 423, 'content': 424, 'justify': 425, 'juvenile': 426, 'information': 427, 'literally': 428, 'spoonfed': 429, 'would': 430, 'hard': 431, 'u': 432, 'instead': 433, 'dialogue': 434, 'poorly': 435, 'write': 436, 'extremely': 437, 'progress': 438, 'likely': 439, 'care': 440, 'hero': 441, 'any': 442, 'jeopardy': 443, 'because': 444, 'youll': 445, 'arent': 446, 'nobody': 447, 'remembers': 448, 'questionable': 449, 'wisdom': 450, 'consider': 451, 'target': 452, 'number': 453, 'memorable': 454, 'can': 455, 'count': 456, 'miss': 457, 'finger': 458, 'two': 459, 'check': 460, 'six': 461, 'clear': 462, 'indication': 463, 'them': 464, 'than': 465, 'attempt': 466, 'cash': 467, 'spending': 468, 'dollar': 469, 'judge': 470, 'rash': 471, 'awful': 472, 'teenflicks': 473, 'avoid': 474, 'cost': 475, 'french': 476, 'boy': 477, 'parent': 478, 'kill': 479, 'before': 480, 'tim': 481, 'roth': 482, 'oops': 483, 'mean': 484, 'evil': 485, 'vow': 486, 'revenge': 487, 'teach': 488, 'musketeer': 489, 'some': 490, 'dude': 491, 'use': 492, 'fourteen': 493, 'arrgh': 494, 'well': 495, 'rest': 496, 'swishswishzzzzzzz': 497, 'critique': 498, 'pretty': 499, 'let': 500, 'start': 501, 'okay': 502, 'story': 503, 'plain': 504, 'original': 505, 'entirely': 506, 'lack': 507, 'energy': 508, 'whats': 509, 'next': 510, 'hmmmm': 511, 'main': 512, 'actor': 513, 'justin': 514, 'chamber': 515, 'basically': 516, 'uncharismatic': 517, 'version': 518, 'chris': 519, 'odonnell': 520, 'less': 521, 'range': 522, 'think': 523, 'about': 524, 'mena': 525, 'suvari': 526, 'off': 527, 'as': 528, 'thora': 529, 'birch': 530, 'dungeon': 531, 'dragon': 532, 'miscast': 533, 'sequence': 534, 'pisspoor': 535, 'im': 536, 'sure': 537, 'ms': 538, 'suvaris': 539, 'fault': 540, 'director': 541, 'ive': 542, 'definitely': 543, 'level': 544, 'semisaving': 545, 'grace': 546, 'actorwise': 547, 'irrepressible': 548, 'guy': 549, 'once': 550, 'again': 551, 'something': 552, 'we': 553, 'havent': 554, 'thousand': 555, 'love': 556, 'god': 557, 'beg': 558, 'agent': 559, 'ask': 560, 'marketplace': 561, 'modern': 562, 'day': 563, 'role': 564, 'romantic': 565, 'good': 566, 'gunk': 567, 'alright': 568, 'what': 569, 'oh': 570, 'yeah': 571, 'score': 572, 'yikes': 573, 'hows': 574, 'down': 575, 'few': 576, 'notch': 577, 'fellas': 578, 'blare': 579, 'ear': 580, 'whenever': 581, 'feel': 582, 'need': 583, 'accentuate': 584, 'certain': 585, 'actually': 586, 'little': 587, 'important': 588, 'behind': 589, 'recognize': 590, 'real': 591, 'epic': 592, 'imagination': 593, 'fluffy': 594, 'rehash': 595, 'cakewalk': 596, 'create': 597, 'shrewd': 598, 'studio': 599, 'advantage': 600, 'kungfu': 601, 'phenomenon': 602, 'test': 603, 'classic': 604, 'dudes': 605, 'around': 606, 'keep': 607, 'read': 608, 'editing': 609, 'shoddy': 610, 'banal': 611, 'stilted': 612, 'problem': 613, 'plentiful': 614, 'top': 615, 'horse': 616, 'carriage': 617, 'stand': 618, 'opponent': 619, 'forever': 620, 'scamper': 621, 'back': 622, 'mouseketeers': 623, 'rope': 624, 'tower': 625, 'jump': 626, 'chord': 627, 'fight': 628, 'him': 629, 'hang': 630, 'anybody': 631, 'later': 632, 'change': 633, 'shirt': 634, 'mind': 635, 'never': 636, 'stray': 637, 'champion': 638, 'sake': 639, 'fun': 640, 'flick': 641, 'acting': 642, 'atrocious': 643, 'lake': 644, 'remind': 645, 'school': 646, 'cringe': 647, 'reason': 648, 'fat': 649, 'give': 650, 'raisondetre': 651, 'through': 652, 'hop': 653, 'pack': 654, 'stuntwork': 655, 'promote': 656, 'trailer': 657, 'essentially': 658, 'snippet': 659, 'major': 660, 'swashbuckling': 661, 'beginning': 662, 'finish': 663, 'juggle': 664, 'ladder': 665, 'itself': 666, 'definite': 667, 'keeper': 668, 'regurgitate': 669, 'crap': 670, 'catherine': 671, 'deneuve': 672, 'place': 673, 'films': 674, 'credit': 675, 'hullo': 676, 'call': 677, 'barely': 678, 'ugh': 679, 'small': 680, 'trash': 681, 'together': 682, 'gang': 683, 'stay': 684, 'thank': 685, 'wheres': 686, 'joblo': 687, 'knight': 688, 'outlaw': 689, 'crouch': 690, 'tiger': 691, 'hidden': 692, 'matrix': 693, 'replacement': 694, 'killer': 695, 'romeo': 696, 'must': 697, 'die': 698, 'shanghai': 699, 'noon': 700, 'synopsis': 701, 'mentally': 702, 'unstable': 703, 'undergo': 704, 'psychotherapy': 705, 'potentially': 706, 'fatal': 707, 'accident': 708, 'fall': 709, 'mother': 710, 'fledgling': 711, 'restauranteur': 712, 'unsuccessfully': 713, 'gain': 714, 'woman': 715, 'favor': 716, 'picture': 717, 'comment': 718, 'stalk': 719, 'yet': 720, 'seemingly': 721, 'endless': 722, 'string': 723, 'spurnedpsychosgettingtheirrevenge': 724, 'type': 725, 'stable': 726, 'category': 727, 'industry': 728, 'theatrical': 729, 'directtovideo': 730, 'proliferation': 731, 'due': 732, 'part': 733, 'theyre': 734, 'typically': 735, 'inexpensive': 736, 'produce': 737, 'special': 738, 'effect': 739, 'serve': 740, 'vehicle': 741, 'flash': 742, 'nudity': 743, 'allow': 744, 'frequent': 745, 'latenight': 746, 'cable': 747, 'waver': 748, 'slightly': 749, 'norm': 750, 'respect': 751, 'psycho': 752, 'affair': 753, 'contrary': 754, 'reject': 755, 'rather': 756, 'exlover': 757, 'exwife': 758, 'exhusband': 759, 'redundant': 760, 'entry': 761, 'doom': 762, 'collect': 763, 'dust': 764, 'shelf': 765, 'view': 766, 'midnight': 767, 'suspense': 768, 'though': 769, 'set': 770, 'intersperse': 771, 'throughout': 772, 'opening': 773, 'instance': 774, 'serioussounding': 775, 'narrator': 776, 'spout': 777, 'statistic': 778, 'stalker': 779, 'ponders': 780, 'cause': 781, 'implicitly': 782, 'imply': 783, 'men': 784, 'snapshot': 785, 'jay': 786, 'underwood': 787, 'state': 788, 'daryl': 789, 'gleason': 790, 'brooke': 791, 'daniel': 792, 'guesswork': 793, 'require': 794, 'proceeds': 795, 'begin': 796, 'obvious': 797, 'contrive': 798, 'quite': 799, 'bit': 800, 'victim': 801, 'obsesses': 802, 'try': 803, 'woo': 804, 'plan': 805, 'desperate': 806, 'alltime': 807, 'psychoinlove': 808, 'cliche': 809, 'murder': 810, 'pet': 811, 'genres': 812, 'dead': 813, 'exception': 814, 'shower': 815, 'event': 816, 'lead': 817, 'inevitable': 818, 'showdown': 819, 'survive': 820, 'guess': 821, 'invariably': 822, 'always': 823, 'conclusion': 824, 'turkey': 825, 'stalkeds': 826, 'uniformly': 827, 'adequate': 828, 'anything': 829, 'home': 830, 'either': 831, 'turn': 832, 'toward': 833, 'melodrama': 834, 'overdo': 835, 'word': 836, 'still': 837, 'manage': 838, 'creepy': 839, 'pass': 840, 'demand': 841, 'maryam': 842, 'dabo': 843, 'close': 844, 'here': 845, 'bond': 846, 'chick': 847, 'living': 848, 'daylight': 849, 'stalked': 850, 'title': 851, 'ditzy': 852, 'strong': 853, 'independent': 854, 'businessowner': 855, 'proceed': 856, 'end': 857, 'example': 858, 'ensure': 859, 'excuse': 860, 'return': 861, 'toolbox': 862, 'house': 863, 'door': 864, 'answer': 865, 'open': 866, 'wanders': 867, 'enter': 868, 'our': 869, 'heroine': 870, 'danger': 871, 'somehow': 872, 'park': 873, 'front': 874, 'oblivious': 875, 'presence': 876, 'inside': 877, 'episode': 878, 'incredible': 879, 'strain': 880, 'suspension': 881, 'question': 882, 'validity': 883, 'receives': 884, 'highly': 885, 'derivative': 886, 'somewhat': 887, 'boring': 888, 'rat': 889, 'mostly': 890, 'several': 891, 'brief': 892, 'strip': 893, 'bar': 894, 'offensive': 895, 'thriller': 896, 'genre': 897, 'youre': 898, 'mood': 899, 'stake': 900, 'quest': 901, 'camelot': 902, 'warner': 903, 'bros': 904, 'featurelength': 905, 'fullyanimated': 906, 'clout': 907, 'disney': 908, 'cartoon': 909, 'empire': 910, 'mouse': 911, 'worry': 912, 'recent': 913, 'challenger': 914, 'throne': 915, 'last': 916, 'falls': 917, 'promising': 918, 'flaw': 919, 'th': 920, 'century': 921, 'fox': 922, 'anastasia': 923, 'hercules': 924, 'lively': 925, 'colorful': 926, 'palate': 927, 'handsdown': 928, 'crown': 929, 'piece': 930, 'animation': 931, 'contest': 932, 'arrival': 933, 'magic': 934, 'kingdom': 935, 'mediocre': 936, 'thatd': 937, 'pocahontas': 938, 'those': 939, 'nearly': 940, 'dull': 941, 'revolve': 942, 'adventure': 943, 'freespirited': 944, 'kayley': 945, 'voice': 946, 'jessalyn': 947, 'gilsig': 948, 'earlyteen': 949, 'daughter': 950, 'belated': 951, 'king': 952, 'arthur': 953, 'round': 954, 'table': 955, 'kayleys': 956, 'dream': 957, 'father': 958, 'footstep': 959, 'chance': 960, 'warlord': 961, 'ruber': 962, 'gary': 963, 'oldman': 964, 'exround': 965, 'membergonebad': 966, 'arthurs': 967, 'magical': 968, 'sword': 969, 'excalibur': 970, 'accidentally': 971, 'lose': 972, 'dangerous': 973, 'boobytrapped': 974, 'forest': 975, 'help': 976, 'hunky': 977, 'blind': 978, 'timberlanddweller': 979, 'garrett': 980, 'carey': 981, 'elwes': 982, 'twoheaded': 983, 'eric': 984, 'idle': 985, 'don': 986, 'rickles': 987, 'argue': 988, 'might': 989, 'able': 990, 'break': 991, 'medieval': 992, 'sexist': 993, 'mold': 994, 'prove': 995, 'fighter': 996, 'side': 997, 'pure': 998, 'showmanship': 999, 'essential': 1000, 'ever': 1001, 'expect': 1002, 'climb': 1003, 'rank': 1004, 'differentiate': 1005, 'youd': 1006, 'saturday': 1007, 'morning': 1008, 'subpar': 1009, 'instantly': 1010, 'forgettable': 1011, 'song': 1012, 'poorlyintegrated': 1013, 'computerized': 1014, 'footage': 1015, 'compare': 1016, 'garretts': 1017, 'runin': 1018, 'angry': 1019, 'ogre': 1020, 'hercs': 1021, 'battle': 1022, 'hydra': 1023, 'case': 1024, 'stink': 1025, 'remotely': 1026, 'interest': 1027, 'race': 1028, 'outbland': 1029, 'others': 1030, 'tie': 1031, 'win': 1032, 'shtick': 1033, 'awfully': 1034, 'cloy': 1035, 'sign': 1036, 'pulse': 1037, 'fan': 1038, 'earlys': 1039, 'tgif': 1040, 'lineup': 1041, 'thrill': 1042, 'jaleel': 1043, 'urkel': 1044, 'white': 1045, 'bronson': 1046, 'balki': 1047, 'pinchot': 1048, 'share': 1049, 'nicely': 1050, 'realize': 1051, 'loss': 1052, 'recall': 1053, 'specific': 1054, 'enthusiastic': 1055, 'pair': 1056, 'singer': 1057, 'musical': 1058, 'moment': 1059, 'jane': 1060, 'seymour': 1061, 'celine': 1062, 'dion': 1063, 'aside': 1064, 'probably': 1065, 'bored': 1066, 'grievous': 1067, 'error': 1068, 'personality': 1069, 'learn': 1070, 'very': 1071, 'yourself': 1072, 'mm': 1073, 'eight': 1074, 'millimeter': 1075, 'wholesome': 1076, 'surveillance': 1077, 'sight': 1078, 'value': 1079, 'enmesh': 1080, 'seedy': 1081, 'sleazy': 1082, 'underworld': 1083, 'hardcore': 1084, 'pornography': 1085, 'business': 1086, 'bubbling': 1087, 'beneath': 1088, 'surface': 1089, 'bigtown': 1090, 'americana': 1091, 'theres': 1092, 'sordid': 1093, 'world': 1094, 'sick': 1095, 'depraved': 1096, 'necessarily': 1097, 'stop': 1098, 'short': 1099, 'order': 1100, 'satisfy': 1101, 'twisted': 1102, 'position': 1103, 'influence': 1104, 'making': 1105, 'kind': 1106, 'demented': 1107, 'want': 1108, 'snuff': 1109, 'suppose': 1110, 'documentary': 1111, 'brutalize': 1112, 'camera': 1113, 'joel': 1114, 'schumacher': 1115, 'run': 1116, 'budget': 1117, 'batman': 1118, 'client': 1119, 'twothirds': 1120, 'unwind': 1121, 'fairly': 1122, 'conventional': 1123, 'person': 1124, 'drama': 1125, 'albeit': 1126, 'particularly': 1127, 'unsavory': 1128, 'core': 1129, 'threaten': 1130, 'along': 1131, 'explode': 1132, 'violence': 1133, 'finally': 1134, 'tag': 1135, 'ridiculous': 1136, 'selfrighteous': 1137, 'finale': 1138, 'drag': 1139, 'unpleasant': 1140, 'experience': 1141, 'far': 1142, 'trust': 1143, 'life': 1144, 'nicolas': 1145, 'snake': 1146, 'cage': 1147, 'private': 1148, 'investigator': 1149, 'welles': 1150, 'hire': 1151, 'wealthy': 1152, 'philadelphia': 1153, 'widow': 1154, 'determine': 1155, 'whether': 1156, 'reel': 1157, 'husband': 1158, 'safe': 1159, 'document': 1160, 'girl': 1161, 'assignment': 1162, 'matteroffactly': 1163, 'puzzle': 1164, 'neatly': 1165, 'almost': 1166, 'specialized': 1167, 'skill': 1168, 'training': 1169, 'easy': 1170, 'obviously': 1171, 'toilet': 1172, 'tank': 1173, 'clue': 1174, 'deep': 1175, 'dig': 1176, 'investigation': 1177, 'obsessed': 1178, 'george': 1179, 'c': 1180, 'scott': 1181, 'paul': 1182, 'schraders': 1183, 'occasionally': 1184, 'flickering': 1185, 'whir': 1186, 'sprocket': 1187, 'wind': 1188, 'projector': 1189, 'task': 1190, 'hint': 1191, 'toll': 1192, 'lovely': 1193, 'keener': 1194, 'frustrate': 1195, 'cleveland': 1196, 'ugly': 1197, 'splitlevel': 1198, 'harrisburg': 1199, 'pa': 1200, 'condemn': 1201, 'condone': 1202, 'subject': 1203, 'exploit': 1204, 'irony': 1205, 'seven': 1206, 'scribe': 1207, 'andrew': 1208, 'kevin': 1209, 'walker': 1210, 'vision': 1211, 'lane': 1212, 'limit': 1213, 'rrated': 1214, 'firstrun': 1215, 'hollywood': 1216, 'product': 1217, 'lot': 1218, 'cover': 1219, 'horror': 1220, 'joaquin': 1221, 'phoenix': 1222, 'bookstore': 1223, 'flunky': 1224, 'max': 1225, 'california': 1226, 'horrid': 1227, 'familiar': 1228, 'revelation': 1229, 'sexual': 1230, 'deviant': 1231, 'indeed': 1232, 'monster': 1233, 'everyday': 1234, 'neither': 1235, 'super': 1236, 'nor': 1237, 'standard': 1238, 'shock': 1239, 'banality': 1240}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: Mostrar el embedding de la palabra \"girl\" (si se encuentra en el vocabulario)\n",
        "palabra = \"girl\"\n",
        "if palabra in embeddings:\n",
        "    print(f\"\\nEmbedding para la palabra '{palabra}':\")\n",
        "    print(embeddings[palabra])\n",
        "else:\n",
        "    print(f\"\\nLa palabra '{palabra}' no se encuentra en el vocabulario.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz4KMklSFD0H",
        "outputId": "a817f307-e177-4ad5-c6f3-1d9c9167aae5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding para la palabra 'girl':\n",
            "[ 1.03391642e-02 -5.36207156e-03  4.44330089e-03  2.00380534e-02\n",
            " -1.30567346e-02 -1.52889667e-02 -1.13486992e-02 -1.31497337e-02\n",
            "  1.25151183e-02  1.52393887e-02  1.96565664e-03 -1.40704457e-02\n",
            "  1.37699293e-02  1.76588986e-02 -3.13013862e-03  6.74422318e-03\n",
            "  4.96512931e-03  1.17316540e-05 -6.74585719e-03 -9.05813370e-03\n",
            " -8.93647224e-03 -5.68999629e-03  1.06144901e-02  1.31454710e-02\n",
            " -1.45915430e-02 -1.81185752e-02 -1.15507934e-03  1.07407784e-02\n",
            "  1.99506455e-03  1.24793760e-02  8.73896480e-03  7.90904928e-03\n",
            "  9.74643161e-04 -1.74114890e-02 -1.34181445e-02  9.56689473e-03\n",
            " -7.43261632e-03  7.38152768e-03 -6.49460964e-03 -1.87230464e-02\n",
            "  6.76638912e-03  1.63569096e-02 -1.05935556e-03 -1.55380545e-02\n",
            " -3.14152497e-03  1.82556584e-02 -6.98845740e-03 -1.60690006e-02\n",
            "  1.12086842e-02 -1.10441456e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l2q06FOQFGOo"
      }
    }
  ]
}