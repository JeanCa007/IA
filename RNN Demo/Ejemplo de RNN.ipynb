{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejemplos de RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejemplo RNN para Predecir la siguiente palabra**\n",
        "\n",
        "**Importar Librerias**"
      ],
      "metadata": {
        "id": "KrysmBCHYBug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Activation\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "-MVmXijVYSyF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hiperparametros**"
      ],
      "metadata": {
        "id": "l7lChroYYeGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_neurons = 50\n",
        "my_optimizer =\"sgd\"\n",
        "batch_size = 60\n",
        "error_function = \"mean_squared_error\"\n",
        "output_nonlinearity = \"softmax\"\n",
        "cycles = 5\n",
        "epochs_per_cycle = 3\n",
        "context = 3"
      ],
      "metadata": {
        "id": "Rg7KA4IKYiIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable hidden_neurons indica cuántas unidades ocultas vamos a utilizar. \n",
        "La variable optimizer define qué optimizador de Keras vamos a utilizar, y en este\n",
        "caso es el descenso de gradiente estocástico\n",
        "La variable batch_size define el tamaño del lote simplemente dice cuántos ejemplos usaremos para\n",
        "una sola iteración del descenso del gradiente estocástico.\n",
        "La variable rror_function = \"mean_squared_error\" le dice a Keras que use el MSE \n",
        "La función de activación output_nonlinearity, es la función de activación softmax o no linealidad,\n",
        "con su nombre Keras \"softmax\".Básicamente transforma un vector z con arbitraria\n",
        "valores reales a un vector con valores que van de 0 a 1, y son tales que\n",
        "todos suman 1."
      ],
      "metadata": {
        "id": "z-Xmks7Ne_PZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data**"
      ],
      "metadata": {
        "id": "h841QdWjZA9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tesla_text_from_file(textfile=\"Hawking.txt\"):\n",
        "  clean_text_chunks = []\n",
        "  with open(textfile, 'r', encoding='utf-8') as text:\n",
        "    for line in text:\n",
        "        clean_text_chunks.append(line)\n",
        "  clean_text = (\"\".join(clean_text_chunks)).lower()\n",
        "  text_as_list = clean_text.split()\n",
        "  return text_as_list\n",
        "text_as_list = create_tesla_text_from_file()"
      ],
      "metadata": {
        "id": "fhz4yvGsZDF9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota: se hace uso de este dataset dado que no se encontro el del Ejemplo del libro**\n",
        "\n",
        "La funcion create_tesla_text_from_file(textfile=\n",
        "\"tesla.txt\")\n",
        "Abre y se lee el archivo, este lo devuelve línea por línea, por lo que\n",
        "Se guarda estas líneas en la variable  clean_text_chunks. \n",
        "Luego se pegan  todas estas juntas en una cadena grande llamada clean_text, y luego se cortan  \n",
        "en palabras individuales y esto  es lo que hace dicha funcion toda la función \n",
        "Y el resultado de todo ese proceso se guarda en text_as_list \n",
        "Ahora, tenemos todo nuestro texto en una lista, donde cada elemento individual es una palabra.\n",
        "\n",
        "**Procesar Data**\n",
        "\n",
        "Tenga en cuenta que puede haber repeticiones de\n",
        "palabras aquí, y eso está perfectamente bien, ya que esto será manejado por la siguiente parte del\n",
        "código:\n"
      ],
      "metadata": {
        "id": "kBjavfnKgPXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_words = set(text_as_list)\n",
        "number_of_words = len(distinct_words)\n",
        "word2index = dict((w, i) for i, w in enumerate(distinct_words))\n",
        "index2word = dict((i, w) for i, w in enumerate(distinct_words))"
      ],
      "metadata": {
        "id": "Of6vSeL_hnIQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable number_of_words cuenta el número de palabras en el texto. \n",
        "La variable word2index crea un diccionario con palabras únicas como claves y su posición en\n",
        "el texto como valores.\n",
        "La variable  index2word hace exactamente lo contrario, crea un diccionario  donde las posiciones son claves y las palabras son valores.\n",
        "\n",
        "Ahora se crea una función que crea una lista de palabras de entrada y\n",
        "una lista de etiquetas de palabras del texto original, que debe tener la forma de una lista de palabras individuales.\n",
        "Lo que buscamos es hacer un\n",
        "estructura 'entrada'/'etiqueta' para predecir la siguiente palabra, y lo hacemos descomponiendo\n",
        "esta oración en una matriz.\n",
        "Esta funcion toma un texto\n",
        "en forma de lista, crea la lista de palabras de entrada y la lista de palabras de etiquetas y devuelve\n",
        "los dos.\n"
      ],
      "metadata": {
        "id": "8UvJS3-Gh2fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_indices_for_text(text_as_list):\n",
        "  input_words = []\n",
        "  label_word = []\n",
        "  for i in range(0,len(text_as_list) - context):\n",
        "    input_words.append((text_as_list[i:i+context]))\n",
        "    label_word.append((text_as_list[i+context]))\n",
        "  return input_words, label_word\n",
        "input_words,label_word = create_word_indices_for_text(text_as_list)\n",
        "\n",
        "input_vectors = np.zeros((len(input_words), context, number_of_words), dtype=np.int16)\n",
        "vectorized_labels = np.zeros((len(input_words), number_of_words), dtype=np.int16)"
      ],
      "metadata": {
        "id": "y9SneB9giQZL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código produce tensores 'en blanco', poblados por ceros.\n",
        "\n",
        "El tensor input_vectors es técnicamente una 'matriz' con tres dimensiones,\n",
        "El vectorized_labels es el mismo, solo  aquí no tenemos tres o n palabras especificadas por la variable context, sino solo\n",
        "una sola, la palabra etiqueta, por lo que necesitamos una dimensión menos en el tensor.\n",
        "\n",
        "Ahora bien necesitamos poner ambos tensores en el lugar apropiado, para esto se hace lo siguiente, donde se busca poiner en 1s los tensores\n"
      ],
      "metadata": {
        "id": "5zdTyveLksDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, input_w in enumerate(input_words):\n",
        "  for j, w in enumerate(input_w):\n",
        "    input_vectors[i, j, word2index[w]] = 1\n",
        "    vectorized_labels[i, word2index[label_word[i]]] = 1"
      ],
      "metadata": {
        "id": "iGiZntlglwzq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo**\n",
        "  Definimos el modelo RNN "
      ],
      "metadata": {
        "id": "MuO24Iwll2Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(hidden_neurons, return_sequences=False,\n",
        "input_shape=(context,number_of_words), unroll=True))\n",
        "model.add(Dense(number_of_words))\n",
        "model.add(Activation(output_nonlinearity))\n",
        "model.compile(loss=error_function, optimizer=my_optimizer)"
      ],
      "metadata": {
        "id": "5zVhjlu-l9B-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todos los parametro que utiliza se definieron antes y se explicaron\n",
        "\n",
        "**Entrenar y Probando el Modelo**\n",
        "\n",
        "Se entrena y se prueba el modelo"
      ],
      "metadata": {
        "id": "bAHXCnermJ9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cycle in range(cycles):\n",
        "  print(\"> − <\" * 50)\n",
        "  print(\"Cycle: %d\" % (cycle+1))\n",
        "  model.fit(input_vectors, vectorized_labels, batch_size = batch_size,\n",
        "epochs = epochs_per_cycle)\n",
        "  test_index = np.random.randint(len(input_words))\n",
        "  test_words = input_words[test_index]\n",
        "  print(\"Generating test from test index %s with words %s:\" % (test_index,\n",
        "test_words))\n",
        "  input_for_test = np.zeros((1, context, number_of_words))\n",
        "  for i, w in enumerate(test_words):\n",
        "    input_for_test[0, i, word2index[w]] = 1\n",
        "  predictions_all_matrix = model.predict(input_for_test, verbose = 0)[0]\n",
        "  predicted_word = index2word[np.argmax(predictions_all_matrix)]\n",
        "  print(\"THE COMPLETE RESULTING SENTENCE IS: %s %s\" % (\"\".join(test_words),\n",
        "predicted_word))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyUYgYpYmREZ",
        "outputId": "f8818444-7394-477c-990e-d6412d712b4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <\n",
            "Cycle: 1\n",
            "Epoch 1/3\n",
            "1141/1141 [==============================] - 50s 44ms/step - loss: 1.0297e-04\n",
            "Epoch 2/3\n",
            "1141/1141 [==============================] - 47s 41ms/step - loss: 1.0297e-04\n",
            "Epoch 3/3\n",
            "1141/1141 [==============================] - 49s 43ms/step - loss: 1.0297e-04\n",
            "Generating test from test index 59217 with words ['que', 'sea', 'posible']:\n",
            "THE COMPLETE RESULTING SENTENCE IS: queseaposible desmoronarían.\n",
            "\n",
            "> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <\n",
            "Cycle: 2\n",
            "Epoch 1/3\n",
            "1141/1141 [==============================] - 47s 42ms/step - loss: 1.0297e-04\n",
            "Epoch 2/3\n",
            "1141/1141 [==============================] - 49s 43ms/step - loss: 1.0297e-04\n",
            "Epoch 3/3\n",
            "1141/1141 [==============================] - 48s 42ms/step - loss: 1.0297e-04\n",
            "Generating test from test index 57175 with words ['velocidades', 'diferentes.', 'esto']:\n",
            "THE COMPLETE RESULTING SENTENCE IS: velocidadesdiferentes.esto dicen\n",
            "\n",
            "> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <\n",
            "Cycle: 3\n",
            "Epoch 1/3\n",
            "1141/1141 [==============================] - 50s 44ms/step - loss: 1.0297e-04\n",
            "Epoch 2/3\n",
            "1141/1141 [==============================] - 48s 42ms/step - loss: 1.0297e-04\n",
            "Epoch 3/3\n",
            "1141/1141 [==============================] - 49s 43ms/step - loss: 1.0297e-04\n",
            "Generating test from test index 53119 with words ['formen', 'ningún', 'tipo']:\n",
            "THE COMPLETE RESULTING SENTENCE IS: formenningúntipo actualidad\n",
            "\n",
            "> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <\n",
            "Cycle: 4\n",
            "Epoch 1/3\n",
            "1141/1141 [==============================] - 50s 44ms/step - loss: 1.0297e-04\n",
            "Epoch 2/3\n",
            "1141/1141 [==============================] - 49s 43ms/step - loss: 1.0297e-04\n",
            "Epoch 3/3\n",
            "1141/1141 [==============================] - 49s 43ms/step - loss: 1.0297e-04\n",
            "Generating test from test index 30295 with words ['las', 'estrellas', 'masivas.']:\n",
            "THE COMPLETE RESULTING SENTENCE IS: lasestrellasmasivas. satisfactorio,\n",
            "\n",
            "> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <> − <\n",
            "Cycle: 5\n",
            "Epoch 1/3\n",
            "1141/1141 [==============================] - 49s 43ms/step - loss: 1.0297e-04\n",
            "Epoch 2/3\n",
            "1141/1141 [==============================] - 51s 45ms/step - loss: 1.0297e-04\n",
            "Epoch 3/3\n",
            "1141/1141 [==============================] - 59s 51ms/step - loss: 1.0297e-04\n",
            "Generating test from test index 52095 with words ['vuelta', 'e', 'ir']:\n",
            "THE COMPLETE RESULTING SENTENCE IS: vueltaeir elípticas.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La idea es entrenar y probar en un ciclo. Un ciclo se compone de un sesión de entrenamiento (con un número de epochs) y luego generamos una oración de prueba a partir del texto y ver si la palabra que da la red tiene sentido cuando se coloca después de las palabras del texto. \n",
        "\n",
        "Esto completa un ciclo.\n",
        "\n",
        "Estos ciclos son acumulativos, y las oraciones se volverán cada vez más significativas después de cada ciclo sucesivo.\n",
        "\n",
        "En los hiperparámetros hemos especificado que entrenaremos durante 5 ciclos, teniendo cada uno 3 epochs por ciclo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VH6WVlQ4qj4o"
      }
    }
  ]
}